{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Croissant in Machine Learning Pipelines ðŸ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Croissant provides a single-file JSON-LD format for Machine Learning (ML) datasets that contains information about data sources, data structure and relevant additional metadata. The standardized format aims to improve the discoverability, accessibility, and interoperability of ML datasets. In this notebook we'll demonstrate using an example croissant file (linked to a dataset from the UKCEH Environment Information Data Centre (EIDC)) in an ML-pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries\n",
    "%%capture --no-display\n",
    "# Install mlcroissant from the source\n",
    "!apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config\n",
    "!pip install \"git+https://github.com/${GITHUB_REPOSITORY:-mlcommons/croissant}.git@${GITHUB_HEAD_REF:-main}#subdirectory=python/mlcroissant&egg=mlcroissant[dev]\"\n",
    "!pip install array_record\n",
    "!pip install tfds-nightly\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlcroissant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from mlcroissant import Dataset\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the underlying data described in the croissant file can be loaded directly using either the [mlcroissant](https://github.com/mlcommons/croissant/tree/main/python/mlcroissant) python library or the [tensorflow croissant builder](https://www.tensorflow.org/datasets/format_specific_dataset_builders#croissantbuilder). Here we'll demonstrate both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data contains values of bare sand area, modelled wind speed, aspect and slope at a 2.5 m spatial resolution for four UK coastal dune fields, Abberfraw (Wales), Ainsdale (England), Morfa Dyffryn (Wales), Penhale (England). Data is stored as a .csv file. Data is available for 620,756.25 m2 of dune at Abberfraw, 550,962.5 m2 of dune at Ainsdale, 1,797,756.25 m2 of dune at Morfa Dyffryn and 2,275,056.25 m2 of dune at Penhale. All values were calculated from aerial imagery and digital terrain models collected between 2014 and 2016.\n",
      "id                int64\n",
      "X               float64\n",
      "Y               float64\n",
      "Aspect          float64\n",
      "Slope           float64\n",
      "WindSpeed       float64\n",
      "BareSand_it1    float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>BareSand_it1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91619</td>\n",
       "      <td>235341.25</td>\n",
       "      <td>368183.75</td>\n",
       "      <td>147.395590</td>\n",
       "      <td>5.947186</td>\n",
       "      <td>1.552845</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91620</td>\n",
       "      <td>235341.25</td>\n",
       "      <td>368181.25</td>\n",
       "      <td>183.392563</td>\n",
       "      <td>7.696039</td>\n",
       "      <td>1.610589</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91621</td>\n",
       "      <td>235341.25</td>\n",
       "      <td>368178.75</td>\n",
       "      <td>174.296432</td>\n",
       "      <td>5.170790</td>\n",
       "      <td>1.567760</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91622</td>\n",
       "      <td>235341.25</td>\n",
       "      <td>368176.25</td>\n",
       "      <td>264.810909</td>\n",
       "      <td>2.708107</td>\n",
       "      <td>1.461571</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91623</td>\n",
       "      <td>235341.25</td>\n",
       "      <td>368173.75</td>\n",
       "      <td>172.195933</td>\n",
       "      <td>11.810599</td>\n",
       "      <td>1.431683</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id          X          Y      Aspect      Slope  WindSpeed  BareSand_it1\n",
       "0  91619  235341.25  368183.75  147.395590   5.947186   1.552845          34.0\n",
       "1  91620  235341.25  368181.25  183.392563   7.696039   1.610589          40.0\n",
       "2  91621  235341.25  368178.75  174.296432   5.170790   1.567760          16.0\n",
       "3  91622  235341.25  368176.25  264.810909   2.708107   1.461571          16.0\n",
       "4  91623  235341.25  368173.75  172.195933  11.810599   1.431683           4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from the croissant file using mlcroissant\n",
    "croissant_file_path = \"../../croissantSpikeZip.json\" #\"../../croissantSpikeZip.json\"\n",
    "dataset = Dataset(jsonld=croissant_file_path)  # Use mlc.Dataset to parse Croissant metadata\n",
    "metadata = dataset.metadata.to_json() # Convert the metadata to a JSON object\n",
    "records = dataset.records(record_set=\"rs-abberfraw\") # Extract records from the dataset\n",
    "print(metadata['description']) # Display the description of the dataset\n",
    "df = pd.DataFrame(records) # Convert the records to a pandas dataframe \n",
    "print(df.dtypes) # Display the datatypes of the columns\n",
    "df[:5] # Display the first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset's description:\n",
      "This data contains values of bare sand area, modelled wind speed, aspect and slope at a 2.5 m spatial resolution for four UK coastal dune fields, Abberfraw (Wales), Ainsdale (England), Morfa Dyffryn (Wales), Penhale (England). Data is stored as a .csv file. Data is available for 620,756.25 m2 of dune at Abberfraw, 550,962.5 m2 of dune at Ainsdale, 1,797,756.25 m2 of dune at Morfa Dyffryn and 2,275,056.25 m2 of dune at Penhale. All values were calculated from aerial imagery and digital terrain models collected between 2014 and 2016.\n",
      "\n",
      "Dataset's citation:\n",
      "Smyth, T.A.G. (2022). Bare sand, wind speed, aspect and slope at four English and Welsh coastal sand dunes, 2014-2016. NERC EDS Environmental Information Data Centre. https://doi.org/10.5285/972599af-0cc3-4e0e-a4dc-2fab7a6dfc85\n",
      "\n",
      "Dataset's features:\n",
      "FeaturesDict({\n",
      "    'Aspect': float32,\n",
      "    'BareSand_it1': float32,\n",
      "    'Slope': float32,\n",
      "    'WindSpeed': float32,\n",
      "    'X': float32,\n",
      "    'Y': float32,\n",
      "    'id': int64,\n",
      "})\n",
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ..\\..\\mlworkflow-examples\\default_examples\\ukceh\\data\\croissant_ukceh\\dunes_data\\rs_abberfraw\\1.0.0...\u001b[0m\n",
      "**************************** WARNING *********************************\n",
      "Warning: The dataset you're trying to generate is using Apache Beam,\n",
      "yet no `beam_runner` nor `beam_options` was explicitly provided.\n",
      "\n",
      "Some Beam datasets take weeks to generate, so are usually not suited\n",
      "for single machine generation. Please have a look at the instructions\n",
      "to setup distributed generation:\n",
      "\n",
      "https://www.tensorflow.org/datasets/beam_datasets#generating_a_beam_dataset\n",
      "**********************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataflow_endpoint DATAFLOW_ENDPOINT]\n",
      "                             [--project PROJECT] [--job_name JOB_NAME]\n",
      "                             [--staging_location STAGING_LOCATION]\n",
      "                             [--temp_location TEMP_LOCATION] [--region REGION]\n",
      "                             [--service_account_email SERVICE_ACCOUNT_EMAIL]\n",
      "                             [--no_auth]\n",
      "                             [--template_location TEMPLATE_LOCATION]\n",
      "                             [--label LABELS] [--update]\n",
      "                             [--transform_name_mapping TRANSFORM_NAME_MAPPING]\n",
      "                             [--enable_streaming_engine]\n",
      "                             [--dataflow_kms_key DATAFLOW_KMS_KEY]\n",
      "                             [--create_from_snapshot CREATE_FROM_SNAPSHOT]\n",
      "                             [--flexrs_goal {COST_OPTIMIZED,SPEED_OPTIMIZED}]\n",
      "                             [--dataflow_service_option DATAFLOW_SERVICE_OPTIONS]\n",
      "                             [--enable_hot_key_logging]\n",
      "                             [--enable_artifact_caching]\n",
      "                             [--impersonate_service_account IMPERSONATE_SERVICE_ACCOUNT]\n",
      "                             [--gcp_oauth_scope GCP_OAUTH_SCOPES]\n",
      "                             [--enable_bucket_read_metric_counter]\n",
      "                             [--enable_bucket_write_metric_counter]\n",
      "                             [--no_gcsio_throttling_counter]\n",
      "                             [--enable_gcsio_blob_generation]\n",
      "ipykernel_launcher.py: error: argument --flexrs_goal: invalid choice: 'c:\\\\Users\\\\jercar\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3d298bca4c4345a6ceea705e7a461812352df5fca.json' (choose from 'COST_OPTIMIZED', 'SPEED_OPTIMIZED')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jercar\\AppData\\Local\\miniconda3\\envs\\croissant-tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the croissant file using tensorflow custom builder\n",
    "builder = tfds.core.dataset_builders.CroissantBuilder(\n",
    "    jsonld=\"../../croissantSpikeZip.json\",\n",
    "    record_set_ids=[\"rs-abberfraw\"],\n",
    "    file_format='array_record',\n",
    "    data_dir=\"../../mlworkflow-examples/default_examples/ukceh/data/croissant_ukceh\",\n",
    ")\n",
    "print(f\"Dataset's description:\\n{builder.info.description}\\n\")\n",
    "print(f\"Dataset's citation:\\n{builder.info.citation}\\n\")\n",
    "print(f\"Dataset's features:\\n{builder.info.features}\")\n",
    "\n",
    "builder.download_and_prepare() # Download and prepare the dataset\n",
    "train,test = builder.as_data_source(split=['default[:75%]','default[75%:]'])\n",
    "\n",
    "print(f\"Train dataset size: {len(train)}\")\n",
    "print(f\"Test dataset size: {len(test)}\")\n",
    "\n",
    "for i in range(5):\n",
    "  print(train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus here is simply to demonstrate that after loading the data from the croissant file it can then easily be put through various ML frameworks. To highlight this we show a scikit-learn pipeline that ingests a pandas dataframe and we show a PyTorch pipeline that ingests the data source provided by the tensorflow croissant builder. It is noted that the specific details of the model and inputs/outputs are not significant and aren't related to a sensible scientific question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sci-kit learn Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines and trains a neural network model to predict the proportion of sand in an image from the wind speed, degree of slope and aspect. The multi-layer perceptron regressor from scikit-learn is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df[[\"WindSpeed\", \"Aspect\", \"Slope\"]]\n",
    "y = df[\"BareSand_it1\"]\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple regression model\n",
    "model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)  # Predictions on training set\n",
    "y_test_pred = model.predict(X_test)  # Predictions on test set\n",
    "\n",
    "# Evaluate the model\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Set - Mean Squared Error: {mse_train}\")\n",
    "print(f\"Training Set - R-squared Value: {r2_train}\")\n",
    "print(f\"Test Set - Mean Squared Error: {mse_test}\")\n",
    "print(f\"Test Set - R-squared Value: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE and R-squared performance metrics are poor, which is as expected as we're not trying to demonstrate sensible scientific research but just how the data can be loaded an put through a ML pipeline using the Croissant format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines and trains a neural network model using PyTorch for a regression task on tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set-up data loaders, which allow us to define batches and the number of samples that will be processed together in one forward and backward pass through the model. In this case, the batch size is set to 128. The RandomSampler is used to randomly sample elements from the training dataset. The num_samples parameter is set to the length of the training dataset, ensuring that all samples are included in each epoch.\n",
    "\n",
    "For the test set no sampler is specified (sampler=None), so the data will be loaded sequentially. The batch_size parameter is also set to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "train_sampler = torch.utils.data.RandomSampler(train, num_samples=len(train)) \n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "    train,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test,\n",
    "    sampler=None,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "  print(batch)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the TabularRegressor class to define a neural network with two hidden layers. The first hidden layer has 64 neurons, and the second hidden layer has 32 neurons. Both layers use the ReLU activation function. The final layer is a linear layer that outputs a single value for regression.\n",
    "\n",
    "The forward method defines the forward pass of the model, where the input features are passed through the hidden layers and activation functions, and finally through the regression layer to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "class TabularRegressor(torch.nn.Module): # Define a simple feedforward neural network\n",
    "    def __init__(self, input_dim): # Define the model's architecture\n",
    "        super(TabularRegressor, self).__init__()  # Call the parent class's constructor\n",
    "        self.hidden1 = torch.nn.Linear(input_dim, 64) # Define the first hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(64, 32) # Define the second hidden layer\n",
    "        self.relu = torch.nn.ReLU() # Define the activation function\n",
    "        self.regressor = torch.nn.Linear(32, 1) # Define the output layer\n",
    "\n",
    "    def forward(self, features): # Define the forward pass\n",
    "        x = self.hidden1(features) # Pass the input through the first hidden layer\n",
    "        x = self.relu(x) # Apply the activation function\n",
    "        x = self.hidden2(x) # Pass the input through the second hidden layer\n",
    "        x = self.relu(x) # Apply the activation function\n",
    "        x = self.regressor(x) # Pass the input through the output layer\n",
    "        return x # Return the output\n",
    "\n",
    "# Extract feature names and target name\n",
    "feature_names = ['Aspect', 'Slope', 'WindSpeed']\n",
    "target_name = 'BareSand_it1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initialized and trained on 5 epochs of the training data. Predictions are then made on the test set and the MSE and R-squared metrics are computed for both sets of data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "input_dim = len(feature_names) # Number of input features\n",
    "model = TabularRegressor(input_dim) # Initialize the model\n",
    "optimizer = torch.optim.Adam(model.parameters()) # Initialize the optimizer\n",
    "loss_function = torch.nn.MSELoss() # Initialize the loss function\n",
    "\n",
    "num_epochs = 5  # number of times the training loop iterates over the whole training data\n",
    "\n",
    "print('Training...')\n",
    "model.train() # Set the model to training mode\n",
    "all_train_targets = []\n",
    "all_train_predictions = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for example in tqdm(train_loader): #  training data is loaded in batches using train_loader\n",
    "        features = torch.stack([example[feature] for feature in feature_names], dim=1).float()\n",
    "        target = example[target_name].unsqueeze(dim=1).float()\n",
    "        prediction = model(features) # input features are passed through the model to obtain predictions.\n",
    "        loss = loss_function(prediction, target) # loss calculated using the MSE loss function\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # gradients are computed using backpropagation\n",
    "        optimizer.step() # model parameters are updated using the optimizer\n",
    "        epoch_loss += loss.item() \n",
    "        all_train_targets.extend(target.squeeze().tolist())\n",
    "        all_train_predictions.extend(prediction.squeeze().tolist())\n",
    "    \n",
    "    epoch_loss /= len(train_loader) # loss for each epoch is calculated\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}') \n",
    "\n",
    "train_r2 = r2_score(all_train_targets, all_train_predictions)\n",
    "print(f'Training R-squared: {train_r2:.4f}')\n",
    "\n",
    "print('Testing...') # Testing the model on the test data\n",
    "model.eval() # Set the model to evaluation mode\n",
    "total_loss = 0 \n",
    "num_examples = 0\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "for example in tqdm(test_loader): # test data is loaded in batches using test_loader\n",
    "    features = torch.stack([example[feature] for feature in feature_names], dim=1).float() \n",
    "    target = example[target_name].unsqueeze(dim=1).float()  \n",
    "    prediction = model(features) # input features are passed through the model to obtain predictions. \n",
    "    loss = loss_function(prediction, target) # loss calculated using the MSE loss function\n",
    "    total_loss += loss.item() * features.shape[0]  \n",
    "    num_examples += features.shape[0] \n",
    "    all_targets.extend(target.squeeze().tolist()) \n",
    "    all_predictions.extend(prediction.squeeze().tolist())\n",
    "\n",
    "mean_squared_error = total_loss / num_examples\n",
    "r2 = r2_score(all_targets, all_predictions)\n",
    "print(f'\\nMean Squared Error: {mean_squared_error:.4f}')\n",
    "print(f'R-squared: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE and R-squared performance metrics are poor, which is as expected as we're not trying to demonstrate sensible scientific research but just how the data can be loaded an put through a ML pipeline using the Croissant format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "croissant-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
