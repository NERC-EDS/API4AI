{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714074be-5222-4063-baee-087d7a1afc26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This shows the work done for the UKCEH croissant spike.\n",
    "\n",
    "The purpose was to demonstrate extracting data from file(s) on our production catalogue (https://catalogue.ceh.ac.uk/) using a hard coded croissant.json file.\n",
    "\n",
    "Only example 1 worked, which shows extraction for a single fileObject.  Examples 2 and 3 show the work I did to try to access netcdf files with and without a login, and also trying to use a **fileSet** - which I now believe is only used against an archive fileObject (or other fileSets that eventually point to an archive fileObject). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c087d-5fdd-4be1-8f69-89050c94c910",
   "metadata": {},
   "source": [
    "Install dependencies used across all cells of investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f1d39-5945-413e-a9b5-1933deb6ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install ipywidgets\n",
    "!pip install mlcroissant\n",
    "!pip install mlcroissant netCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e56f7c-8c71-444e-af9d-4bfd70a9a9f3",
   "metadata": {},
   "source": [
    "## Example 1 - successfully access a single csv file from archived and unarchived downloads\n",
    "This demonstrates extracting columns from csv files that are downloaded as archived (zip) and unarchived (raw csv) files.\n",
    "\n",
    "All files are freely available and need no login.\n",
    "\n",
    "All data available via OGL license.\n",
    "\n",
    "1. Archived\n",
    "   croissantSpikeZip.json is the croissant file for an archived dataset that is downloaded as a zip file.\n",
    "   Full information for this dataset is [here](https://catalogue.ceh.ac.uk/documents/972599af-0cc3-4e0e-a4dc-2fab7a6dfc85).\n",
    "   It contains 4 csv files for sand dune data, with up to ~30,000 rows per file.\n",
    "\n",
    "2. Unarchived\n",
    "   croissantSpikeCOSMOSSingle.json is a croissant file that points to a couple of raw csv download files.\n",
    "   Full information for this dataset is [here](https://catalogue.ceh.ac.uk/documents/399ed9b1-bf59-4d85-9832-ee4d29f49bfb)\n",
    "   Just a couple of csv's are used from the full set of about 1300 csv links available [here](https://catalogue.ceh.ac.uk/datastore/eidchub/399ed9b1-bf59-4d85-9832-ee4d29f49bfb/).\n",
    "\n",
    "I have not yet worked out how to extract data from subsets of csv files in the archive (zip) file using a 'fileSet' and glob pattern  - this would be very useful for archive files that contain many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "ee8e779c-4157-4c7c-bf6b-dc3f7023b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 91619, 'X': 235341.25, 'Y': 368183.75, 'Aspect': 147.3955898, 'Slope': 5.947185636, 'WindSpeed': 1.552845}\n",
      "{'id': 91620, 'X': 235341.25, 'Y': 368181.25, 'Aspect': 183.3925629, 'Slope': 7.696038723, 'WindSpeed': 1.6105891}\n",
      "{'id': 91621, 'X': 235341.25, 'Y': 368178.75, 'Aspect': 174.296432, 'Slope': 5.170789957, 'WindSpeed': 1.5677601}\n",
      "{'id': 91622, 'X': 235341.25, 'Y': 368176.25, 'Aspect': 264.8109093, 'Slope': 2.708107293, 'WindSpeed': 1.4615709}\n",
      "{'id': 91623, 'X': 235341.25, 'Y': 368173.75, 'Aspect': 172.1959329, 'Slope': 11.81059909, 'WindSpeed': 1.4316834}\n",
      "{'id': 91624, 'X': 235341.25, 'Y': 368171.25, 'Aspect': 255.1948204, 'Slope': 7.329361081, 'WindSpeed': 1.814988}\n",
      "{'id': 91625, 'X': 235341.25, 'Y': 368168.75, 'Aspect': 206.2561874, 'Slope': 6.884741783, 'WindSpeed': 1.6822661}\n",
      "{'id': 91626, 'X': 235341.25, 'Y': 368166.25, 'Aspect': 237.1464386, 'Slope': 8.261778951, 'WindSpeed': 1.4579354}\n",
      "{'id': 91627, 'X': 235341.25, 'Y': 368163.75, 'Aspect': 240.042305, 'Slope': 5.523819923, 'WindSpeed': 1.4785903}\n",
      "{'id': 91628, 'X': 235341.25, 'Y': 368161.25, 'Aspect': 270.6544495, 'Slope': 10.4367075, 'WindSpeed': 1.2462929}\n",
      "{'id': 91629, 'X': 235341.25, 'Y': 368158.75, 'Aspect': 281.7114868, 'Slope': 14.4877305, 'WindSpeed': 0.99423441}\n",
      "{'id': 92159, 'X': 235343.75, 'Y': 368193.75, 'Aspect': 257.6832695, 'Slope': 11.59320092, 'WindSpeed': 1.5860065}\n",
      "{'lwin': -9999.0, 'lwout': -9999.0}\n",
      "{'lwin': 28.5, 'lwout': 34.1}\n",
      "{'lwin': 30.8, 'lwout': 32.8}\n",
      "{'lwin': 29.7, 'lwout': 33.6}\n",
      "{'lwin': 29.0, 'lwout': 33.9}\n",
      "{'lwin': 31.6, 'lwout': 32.9}\n",
      "{'lwin': 28.3, 'lwout': 34.4}\n",
      "{'lwin': 28.5, 'lwout': 31.7}\n",
      "{'lwin': 28.3, 'lwout': 32.5}\n",
      "{'lwin': 29.0, 'lwout': 32.9}\n",
      "{'lwin': 30.5, 'lwout': 32.1}\n",
      "{'lwin': 29.6, 'lwout': 33.3}\n"
     ]
    }
   ],
   "source": [
    "import mlcroissant as mlc\n",
    "\n",
    "def doML(url, recordsetId):\n",
    "    dataset = mlc.Dataset(jsonld=url)\n",
    "    \n",
    "    dataset = mlc.Dataset(jsonld=url)\n",
    "    records = dataset.records(record_set=recordsetId)\n",
    "    for i, record in enumerate(records):\n",
    "      print(record)\n",
    "      if i > 10:\n",
    "        break\n",
    "      \n",
    "doML('croissantSpikeZip.json', 'rs-abberfraw')  # This one demostrates accessing a zip file containing 4 csv files and extracting a set of columns from one of them\n",
    "\n",
    "doML('croissantSpikeCOSMOSSingle.json', 'rs-one-file') # This one demonstrates acccessing columns of a single csv downloaded directly ie not in an archive file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a826db0-7edc-4add-85bc-73a66147d4b0",
   "metadata": {},
   "source": [
    "## Example 2 - fail to handle freely available netcdf files\n",
    "This example shows the work I did to try (and fail) to access netcdf files that are freely available - no login required.\n",
    "\n",
    "It tries to access the netcdf files of the Hadukgrid dataset, which contains links to many netcdf files [here](https://catalogue.ceh.ac.uk/datastore/eidchub/beb62085-ba81-480c-9ed0-2d31c27ff196/).\n",
    "\n",
    "Files have to be downloaded individually as there is no archive file available.\n",
    "\n",
    "I could not work out how to define a netcdf 'dataType' and get the 'extract' working in the recordSet, also I don't think I've got the fileSet correct.  In hindsight I could change it to access individual netcdf files successfully (as in example 1), but still not handle the netcdf format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "5e644f73-ec4e-4572-8ed0-224029837fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'source', 'column', 'conformsTo', 'extract', 'md5', 'jsonPath', 'includes', 'parentField', 'transform', 'isLiveDataset', 'subField', 'fileProperty', 'key', 'path', 'format', 'regex', 'data', 'examples', 'fileObject', 'repeated', 'fileSet', 'replace', 'separator', 'references'}\n",
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'source', 'column', 'conformsTo', 'extract', 'md5', 'jsonPath', 'includes', 'parentField', 'transform', 'isLiveDataset', 'subField', 'fileProperty', 'key', 'path', 'format', 'regex', 'data', 'examples', 'fileObject', 'repeated', 'fileSet', 'replace', 'separator', 'references'}\n"
     ]
    }
   ],
   "source": [
    "import mlcroissant as mlc\n",
    "import netCDF4 as nc\n",
    "   \n",
    "def doML(url, recordsetId):\n",
    "    dataset = mlc.Dataset(jsonld=url)\n",
    "    \n",
    "    dataset = mlc.Dataset(jsonld=url)\n",
    "    records = dataset.records(record_set=recordsetId)\n",
    "    for i, record in enumerate(records): # <-- It fails here because I can't workout what the 'dataType' should be nor the 'extract'  should be in the recordSet of the croissant file - time to use an easier csv example dataset!\n",
    "      print(record)\n",
    "      if i > 10:\n",
    "        break\n",
    "      \n",
    "doML('croissantSpikeHadukgrid.json', 'rs/file-set-dtr-preOct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d35abe-dbcb-4115-be54-4aa1b04c140e",
   "metadata": {},
   "source": [
    "## Example 3 - fail to access netcdf files protected by login\n",
    "This example shows the work I did to try to access CHESS netcdf files that require a login to access the files.\n",
    "\n",
    "It shows that credentials or a session token IS NOT supported.\n",
    "\n",
    "Instead, I show how to traverse the json file and access the files - which is not really in the spirit of using croissant!\n",
    "\n",
    "The source files are hierachically organised, starting [here](https://catalogue.ceh.ac.uk/datastore/eidchub/835a50df-e74f-4bfb-b593-804fd61d5eab/)\n",
    "Creating an account for the login is [here](https://catalogue.ceh.ac.uk/sso/signup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95113bdd-072a-4ef2-9798-d53d3d7d672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import json\n",
    "import mlcroissant as mlc\n",
    "import fnmatch\n",
    "import netCDF4 as nc\n",
    "\n",
    "# Download a file from a url using credentials (specified in 'session' parameter)\n",
    "def download_file(session, file_url, save_path):\n",
    "    response = session.get(file_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Save the file locally\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File downloaded successfully: {save_path}\")\n",
    "    else:\n",
    "        print(\"Failed to download the file. Status code:\", response.status_code)\n",
    "\n",
    "# Function to get a key's value from a fileSet\n",
    "def get_fileset_key_value(fileset_id, key, metadata):\n",
    "    for item in metadata.get(\"distribution\", []):\n",
    "        if item[\"@id\"] == fileset_id:\n",
    "            return item.get(key)\n",
    "    return None\n",
    "\n",
    "# Get all the file download urls for all the fileObjects in the 'distribution' that match the pattern\n",
    "def get_file_urls(pattern, metadata):\n",
    "    objects = metadata.get(\"distribution\", [])\n",
    "    return [obj.get('contentUrl') for obj in objects if obj['@type'] == 'cr:FileObject' and fnmatch.fnmatch(obj.get('contentUrl'), pattern)]\n",
    "   \n",
    "# Do the machine learning stuff - ie read a croissant file and download files from a RecordSet\n",
    "# IMPORTANT: the implementation has been painful because mlcroissant doesn't support credentials or session tokens\n",
    "# This meant that most of the time is spent traversing json (sigh!) rather than using the convenient methods of mlcroissant\n",
    "# See block of comments lower down to see how easy it could otherwise be\n",
    "def doML(session):\n",
    "    url = \"croissantSpikeChess.json\"\n",
    "    dataset = mlc.Dataset(jsonld=url)\n",
    "    \n",
    "    # # Since you can't use mscroissant to automagically get the files (see login issue mentioned below).\n",
    "    # # then you have to get croissant's metadata and loop through them yourselves (sigh!)\n",
    "    metadata = dataset.metadata.to_json()\n",
    "    record_sets = metadata.get(\"recordSet\",[])\n",
    "    # print(record_sets)\n",
    "    for record_set in record_sets:\n",
    "        if(record_set['@id'] == 'rs/file-set-dtr-january1961'):\n",
    "            field = record_set['field'][0]\n",
    "            sourceFileset = field['cr:source']['cr:fileSet']['@id'] # <-- I know it is a cr:fileSet and not cr:fileObject, should really test\n",
    "            # Now get the glob that defines the fileObjects to return\n",
    "            includes = get_fileset_key_value(sourceFileset, 'cr:includes', metadata)\n",
    "\n",
    "            # Now get required netcdf files using the 'includes' glob\n",
    "            file_urls = get_file_urls(f'*{includes}', metadata)\n",
    "\n",
    "            # Download the files from the urls\n",
    "            for file_url in file_urls:\n",
    "                filename = file_url.rsplit('/', 1)[-1]\n",
    "                download_file(session, file_url, filename)\n",
    "\n",
    "            # Use the files\n",
    "            for file_url in file_urls:\n",
    "                filename = file_url.rsplit('/', 1)[-1]\n",
    "                ncdataset = nc.Dataset(filename, 'r')\n",
    "                # Print the dimensions\n",
    "                for dim_name, dim in ncdataset.dimensions.items():\n",
    "                    print(f\"{dim_name}: {len(dim)}\")\n",
    "                # Print the variables\n",
    "                print(\"\\nVariables:\")\n",
    "                for var_name, var in ncdataset.variables.items():\n",
    "                    print(f\"{var_name}: {var.dimensions}\")\n",
    "                # Print some data\n",
    "                # Query some data\n",
    "                # For example, extracting data from a variable named 'temperature'\n",
    "                # temperature_data = dataset.variables['temperature'][:]\n",
    "\n",
    "\n",
    "                \n",
    "# # Typically this is how you would access data using mlcroissant\n",
    "# # However, mlcroissant DOES NOT SUPPORT CREDENTIALS OR AUTH TOKENS\n",
    "# # Also, it automatically tries to access the datasets when using 'dataset.records()' - 2nd line below\n",
    "# # So, it 'automatically' fails since it is trying to access endpoints protected by authentication it can't handle\n",
    "# # This makes using mlcroissant problematic if there isn't a solution I've missed\n",
    "# # I think ML libraries that make use of mlcroissant will not work because of this\n",
    "# # Instead we need to traverse the json of 'dataset' object - which is the solution implemented above\n",
    "# dataset = mlc.Dataset(jsonld=\"phils-croissant-doctored.json\")\n",
    "# records = dataset.records(record_set=\"file-set-dtr-january1961\") # This line tries to call the endpoint without authentication\n",
    "# for i, record in enumerate(records):\n",
    "#   print(record)\n",
    "#   if i > 10:\n",
    "#     break\n",
    "      \n",
    "# Create widgets for username and password\n",
    "username = widgets.Text(description='Username:')\n",
    "password = widgets.Password(description='Password:')\n",
    "login_button = widgets.Button(description='Login')\n",
    "\n",
    "# Get user credentials and pass to the machine learning function\n",
    "def login(button):\n",
    "    requests.Session()\n",
    "    user = username.value\n",
    "    pwd = password.value\n",
    "    session.auth = (username.value, password.value)\n",
    "    doML(session)\n",
    "\n",
    "# Attach the login function to the button\n",
    "login_button.on_click(login)\n",
    "\n",
    "# Display the widgets\n",
    "display(\"Login to UKCEH's catalogue to download data.  If required, create an account at https://catalogue.ceh.ac.uk/sso/signup\", username, password, login_button)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
