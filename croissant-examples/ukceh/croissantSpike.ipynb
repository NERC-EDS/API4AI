{
  "cells": [
   {
    "cell_type": "markdown",
    "id": "714074be-5222-4063-baee-087d7a1afc26",
    "metadata": {},
    "source": [
     "# Introduction\n",
     "\n",
     "This shows the work done for the UKCEH croissant spike.\n",
     "\n",
     "The purpose was to demonstrate extracting data from file(s) on our production catalogue (https://catalogue.ceh.ac.uk/) using a hard coded croissant.json file.\n",
     "\n",
     "Only example 1 worked, which shows extraction for a single fileObject.  Examples 2 and 3 show the work I did to try to access netcdf files with and without a login, and also trying to use a **fileSet** - which I now believe is only used against an archive fileObject (or other fileSets that eventually point to an archive fileObject). "
    ]
   },
   {
    "cell_type": "markdown",
    "id": "c11c087d-5fdd-4be1-8f69-89050c94c910",
    "metadata": {},
    "source": [
     "Install dependencies used across all cells of investigation."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "e97f1d39-5945-413e-a9b5-1933deb6ccc7",
    "metadata": {},
    "outputs": [],
    "source": [
     "!pip install requests\n",
     "!pip install ipywidgets\n",
     "!pip install mlcroissant\n",
     "!pip install mlcroissant netCDF4"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "04e56f7c-8c71-444e-af9d-4bfd70a9a9f3",
    "metadata": {},
    "source": [
     "## Example 1 - successfully access a single csv file\n",
     "This demonstrates the download and extraction of data from a COSMOS csv file that is available on UKCEH's production catalogue.\n",
     "\n",
     "There are links to 1300 COSMOS csv files [here](https://catalogue.ceh.ac.uk/datastore/eidchub/399ed9b1-bf59-4d85-9832-ee4d29f49bfb/).\n",
     "\n",
     "These files are freely available and need no login.\n",
     "\n",
     "I have not worked out how to access them using a glob pattern using a FileSet, and I think all files need to be downloaded as a single archive file (zip), which means you can't be selective about which files you download when doing ML - ie you have to download them all in an archive file first (I hope I'm wrong)."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "ee8e779c-4157-4c7c-bf6b-dc3f7023b0e1",
    "metadata": {},
    "outputs": [],
    "source": [
     "import mlcroissant as mlc\n",
     "\n",
     "def doML(url, recordsetId):\n",
     "    dataset = mlc.Dataset(jsonld=url)\n",
     "    \n",
     "    dataset = mlc.Dataset(jsonld=url)\n",
     "    records = dataset.records(record_set=recordsetId)\n",
     "    print(records)\n",
     "    for i, record in enumerate(records):\n",
     "      print(record)\n",
     "      if i > 10:\n",
     "        break\n",
     "      \n",
     "doML('croissantSpikeCOSMOSSingle.json', 'rs-one-file')\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "4a826db0-7edc-4add-85bc-73a66147d4b0",
    "metadata": {},
    "source": [
     "## Example 2 - fail to handle freely available netcdf files\n",
     "This example shows the work I did to try (and fail) to access netcdf files that are freely available - no login required.\n",
     "\n",
     "It tries to access the netcdf files of the Hadukgrid dataset, which contains links to many netcdf files [here](https://catalogue.ceh.ac.uk/datastore/eidchub/beb62085-ba81-480c-9ed0-2d31c27ff196/).\n",
     "\n",
     "Files have to be downloaded individually as there is no archive file available.\n",
     "\n",
     "I could not work out how to define a netcdf 'dataType' and get the 'extract' working in the recordSet, also I don't think I've got the fileSet correct.  In hindsight I could change it to access individual netcdf files successfully (as in example 1), but still not handle the netcdf format.\n"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "5e644f73-ec4e-4572-8ed0-224029837fca",
    "metadata": {},
    "outputs": [],
    "source": [
     "import mlcroissant as mlc\n",
     "import netCDF4 as nc\n",
     "   \n",
     "def doML(url, recordsetId):\n",
     "    dataset = mlc.Dataset(jsonld=url)\n",
     "    \n",
     "    dataset = mlc.Dataset(jsonld=url)\n",
     "    records = dataset.records(record_set=recordsetId)\n",
     "    for i, record in enumerate(records): # <-- It fails here because I can't workout what the 'dataType' should be nor the 'extract'  should be in the recordSet of the croissant file - time to use an easier csv example dataset!\n",
     "      print(record)\n",
     "      if i > 10:\n",
     "        break\n",
     "      \n",
     "doML('croissantSpikeHadukgrid.json', 'rs/file-set-dtr-preOct')\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "74d35abe-dbcb-4115-be54-4aa1b04c140e",
    "metadata": {},
    "source": [
     "## Example 3 - fail to access netcdf files protected by login\n",
     "This example shows the work I did to try to access CHESS netcdf files that require a login to access the files.\n",
     "\n",
     "It shows that credentials or a session token IS NOT supported.\n",
     "\n",
     "Instead, I show how to traverse the json file and access the files - which is not really in the spirit of using croissant!\n",
     "\n",
     "The source files are hierachically organised, starting [here](https://catalogue.ceh.ac.uk/datastore/eidchub/835a50df-e74f-4bfb-b593-804fd61d5eab/)\n",
     "Creating an account for the login is [here](https://catalogue.ceh.ac.uk/sso/signup)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "95113bdd-072a-4ef2-9798-d53d3d7d672a",
    "metadata": {},
    "outputs": [],
    "source": [
     "import ipywidgets as widgets\n",
     "from IPython.display import display\n",
     "import requests\n",
     "import json\n",
     "import mlcroissant as mlc\n",
     "import fnmatch\n",
     "import netCDF4 as nc\n",
     "\n",
     "# Download a file from a url using credentials (specified in 'session' parameter)\n",
     "def download_file(session, file_url, save_path):\n",
     "    response = session.get(file_url)\n",
     "    \n",
     "    # Check if the request was successful\n",
     "    if response.status_code == 200:\n",
     "        # Save the file locally\n",
     "        with open(save_path, \"wb\") as file:\n",
     "            file.write(response.content)\n",
     "        print(f\"File downloaded successfully: {save_path}\")\n",
     "    else:\n",
     "        print(\"Failed to download the file. Status code:\", response.status_code)\n",
     "\n",
     "# Function to get a key's value from a fileSet\n",
     "def get_fileset_key_value(fileset_id, key, metadata):\n",
     "    for item in metadata.get(\"distribution\", []):\n",
     "        if item[\"@id\"] == fileset_id:\n",
     "            return item.get(key)\n",
     "    return None\n",
     "\n",
     "# Get all the file download urls for all the fileObjects in the 'distribution' that match the pattern\n",
     "def get_file_urls(pattern, metadata):\n",
     "    objects = metadata.get(\"distribution\", [])\n",
     "    return [obj.get('contentUrl') for obj in objects if obj['@type'] == 'cr:FileObject' and fnmatch.fnmatch(obj.get('contentUrl'), pattern)]\n",
     "   \n",
     "# Do the machine learning stuff - ie read a croissant file and download files from a RecordSet\n",
     "# IMPORTANT: the implementation has been painful because mlcroissant doesn't support credentials or session tokens\n",
     "# This meant that most of the time is spent traversing json (sigh!) rather than using the convenient methods of mlcroissant\n",
     "# See block of comments lower down to see how easy it could otherwise be\n",
     "def doML(session):\n",
     "    url = \"croissantSpikeChess.json\"\n",
     "    dataset = mlc.Dataset(jsonld=url)\n",
     "    \n",
     "    # # Since you can't use mscroissant to automagically get the files (see login issue mentioned below).\n",
     "    # # then you have to get croissant's metadata and loop through them yourselves (sigh!)\n",
     "    metadata = dataset.metadata.to_json()\n",
     "    record_sets = metadata.get(\"recordSet\",[])\n",
     "    # print(record_sets)\n",
     "    for record_set in record_sets:\n",
     "        if(record_set['@id'] == 'rs/file-set-dtr-january1961'):\n",
     "            field = record_set['field'][0]\n",
     "            sourceFileset = field['cr:source']['cr:fileSet']['@id'] # <-- I know it is a cr:fileSet and not cr:fileObject, should really test\n",
     "            # Now get the glob that defines the fileObjects to return\n",
     "            includes = get_fileset_key_value(sourceFileset, 'cr:includes', metadata)\n",
     "\n",
     "            # Now get required netcdf files using the 'includes' glob\n",
     "            file_urls = get_file_urls(f'*{includes}', metadata)\n",
     "\n",
     "            # Download the files from the urls\n",
     "            for file_url in file_urls:\n",
     "                filename = file_url.rsplit('/', 1)[-1]\n",
     "                download_file(session, file_url, filename)\n",
     "\n",
     "            # Use the files\n",
     "            for file_url in file_urls:\n",
     "                filename = file_url.rsplit('/', 1)[-1]\n",
     "                ncdataset = nc.Dataset(filename, 'r')\n",
     "                # Print the dimensions\n",
     "                for dim_name, dim in ncdataset.dimensions.items():\n",
     "                    print(f\"{dim_name}: {len(dim)}\")\n",
     "                # Print the variables\n",
     "                print(\"\\nVariables:\")\n",
     "                for var_name, var in ncdataset.variables.items():\n",
     "                    print(f\"{var_name}: {var.dimensions}\")\n",
     "                # Print some data\n",
     "                # Query some data\n",
     "                # For example, extracting data from a variable named 'temperature'\n",
     "                # temperature_data = dataset.variables['temperature'][:]\n",
     "\n",
     "\n",
     "                \n",
     "# # Typically this is how you would access data using mlcroissant\n",
     "# # However, mlcroissant DOES NOT SUPPORT CREDENTIALS OR AUTH TOKENS\n",
     "# # Also, it automatically tries to access the datasets when using 'dataset.records()' - 2nd line below\n",
     "# # So, it 'automatically' fails since it is trying to access endpoints protected by authentication it can't handle\n",
     "# # This makes using mlcroissant problematic if there isn't a solution I've missed\n",
     "# # I think ML libraries that make use of mlcroissant will not work because of this\n",
     "# # Instead we need to traverse the json of 'dataset' object - which is the solution implemented above\n",
     "# dataset = mlc.Dataset(jsonld=\"phils-croissant-doctored.json\")\n",
     "# records = dataset.records(record_set=\"file-set-dtr-january1961\") # This line tries to call the endpoint without authentication\n",
     "# for i, record in enumerate(records):\n",
     "#   print(record)\n",
     "#   if i > 10:\n",
     "#     break\n",
     "      \n",
     "# Create widgets for username and password\n",
     "username = widgets.Text(description='Username:')\n",
     "password = widgets.Password(description='Password:')\n",
     "login_button = widgets.Button(description='Login')\n",
     "\n",
     "# Get user credentials and pass to the machine learning function\n",
     "def login(button):\n",
     "    requests.Session()\n",
     "    user = username.value\n",
     "    pwd = password.value\n",
     "    session.auth = (username.value, password.value)\n",
     "    doML(session)\n",
     "\n",
     "# Attach the login function to the button\n",
     "login_button.on_click(login)\n",
     "\n",
     "# Display the widgets\n",
     "display(\"Login to UKCEH's catalogue to download data.  If required, create an account at https://catalogue.ceh.ac.uk/sso/signup\", username, password, login_button)\n"
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.9"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
 